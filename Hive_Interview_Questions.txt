1. What is the definition of Hive? What is the present version of Hive? 
Ans: 
Hive is a data warehousing and SQL-like query language software project that facilitates querying and managing large datasets stored in distributed storage systems like Hadoop Distributed File System (HDFS). It was initially developed by Facebook and later became an open-source Apache project.

Hive provides a SQL-like interface to query data stored in Hadoop clusters and supports a wide range of analytical functions like filtering, aggregating, and joining data. It also supports user-defined functions (UDFs) and allows users to define custom mappers and reducers to process data in a distributed manner.

Present version : 4.0. 0-alpha-2

2. Is Hive suitable to be used for OLTP systems? Why?
Ans:
No, Hive is not suitable for OLTP (Online Transaction Processing) systems.

OLTP systems are designed to handle high transaction volumes with low latency and require fast and frequent updates to individual records in a database. Hive, on the other hand, is designed for OLAP (Online Analytical Processing) workloads, which involve running complex analytical queries on large datasets.

Hive is optimized for running long-running batch jobs that perform large-scale data processing and analysis. It is not optimized for low-latency, real-time transaction processing. Hive's query execution engine is optimized for scanning and aggregating large amounts of data, but it may not be suitable for handling individual record-level updates in real-time.

Therefore, if you have OLTP workloads that require fast and frequent updates to individual records, it would be more appropriate to use a different database system designed specifically for OLTP workloads, such as MySQL or Oracle.


3. How is HIVE different from RDBMS? Does hive support ACID transactions. If not then give the proper reason
Ans:
Hive is different from traditional RDBMS (Relational Database Management System) in several ways:

Data Model: RDBMS follows a structured schema-on-write approach, where data is stored in tables with a predefined schema. Hive, on the other hand, follows a schema-on-read approach, where the schema is inferred when the data is queried.

Query Language: RDBMS typically uses SQL as the query language, whereas Hive uses a SQL-like query language called HiveQL (HQL).

Scalability: RDBMS are designed to run on a single server, whereas Hive is designed to run on a distributed computing system like Hadoop, making it highly scalable.

Data Types: RDBMS typically supports a limited set of data types, whereas Hive supports a wide range of data types including complex data types like arrays and maps.

Regarding ACID (Atomicity, Consistency, Isolation, Durability) transactions, Hive does not support full ACID transactions. Although, it does support limited transactional capabilities since version 0.13, called ACID tables. ACID tables in Hive are tables that support atomicity and durability guarantees for transactions involving data inserts, updates, and deletes. However, it does not support consistency and isolation guarantees.

The reason why Hive does not support full ACID transactions is that it is designed primarily for analytical workloads, which are typically batch-oriented and read-heavy. ACID transactions impose overhead on write operations, which may affect Hive's performance and scalability on large datasets. Therefore, Hive's design prioritizes scalability and query performance over transactional guarantees.

4. Explain the hive architecture and the different components of a Hive architecture?
Ans:
Hive architecture consists of several components that work together to enable data warehousing and analytics on large datasets stored in distributed storage systems. The different components of the Hive architecture are:

Metastore: The Metastore is a centralized metadata repository that stores schema information and other metadata about tables, partitions, and columns. It is responsible for managing the mapping between the logical schema and the physical data stored in Hadoop Distributed File System (HDFS) or other storage systems.

Driver: The Driver is responsible for parsing and executing user queries. It accepts HiveQL queries from users, translates them into a series of MapReduce or Spark jobs, and coordinates the execution of those jobs.

Execution Engine: The Execution Engine is responsible for executing the tasks generated by the Driver. It includes several execution modes, such as MapReduce, Spark, Tez, and LLAP (Live Long And Process). The execution engine is responsible for distributing the workload across a cluster of nodes and optimizing query performance.

HCatalog: HCatalog is a table and storage management layer for Hadoop that provides a metadata abstraction layer for Hive, Pig, and MapReduce. It enables users to create, manage, and share data across different Hadoop components and applications.

SerDe: SerDe (Serializer/Deserializer) is a mechanism that allows Hive to read and write data in various formats such as CSV, JSON, Avro, and Parquet. SerDe provides a way to serialize data into a format that can be stored in Hadoop and deserialized into a Hive table.

Storage Handler: The Storage Handler is responsible for managing the data storage and retrieval for Hive tables. It provides a mechanism to store data in different storage systems, including HDFS, HBase, and Amazon S3.

Overall, Hive architecture is designed to handle large-scale data warehousing and analytical workloads on distributed storage systems. Its components work together to provide a SQL-like interface to query and manage data stored in Hadoop clusters or other distributed storage systems.


5. Mention what Hive query processor does? And Mention what are the components of a Hive query processor?
Ans:

The Hive query processor is responsible for parsing, analyzing, optimizing, and executing HiveQL queries on data stored in Hadoop or other distributed storage systems. The query processor comprises several components that work together to process user queries:

Parser: The Parser is responsible for parsing HiveQL queries and converting them into an internal query representation that can be further processed by other components.

Semantic Analyzer: The Semantic Analyzer performs semantic checks on the parsed query to ensure that it conforms to the HiveQL syntax and is semantically correct. It also resolves references to database objects such as tables, columns, and functions.

Query Optimizer: The Query Optimizer analyzes the query plan generated by the Semantic Analyzer and optimizes it for better performance. It considers different optimization techniques such as query rewriting, join reordering, and predicate pushdown to generate an optimal query plan.

Physical Plan Generator: The Physical Plan Generator is responsible for generating a physical query plan from the optimized logical query plan. It determines the optimal data access methods, data partitioning, and data distribution techniques to execute the query efficiently.

Execution Engine: The Execution Engine executes the physical query plan generated by the Physical Plan Generator. It converts the plan into a set of MapReduce, Spark, or Tez jobs, which are then executed on a Hadoop cluster.

Result Set Formatter: The Result Set Formatter is responsible for formatting the query result set into a readable and interpretable format such as CSV, JSON, or HTML.

Overall, the Hive query processor is a complex and powerful system that enables users to query and analyze large datasets stored in distributed storage systems. Its components work together to provide an efficient and optimized query processing environment for HiveQL queries.

6. What are the three different modes in which we can operate Hive?
Ans:

Hive can operate in three different modes:

Local Mode: In local mode, Hive runs in a single JVM on a local machine. It is used for development and testing purposes when working with small datasets.

MapReduce Mode: In MapReduce mode, Hive uses Hadoop MapReduce to execute queries on data stored in Hadoop Distributed File System (HDFS). This mode is suitable for large-scale data processing and analysis on distributed systems.

Spark Mode: In Spark mode, Hive uses Apache Spark as an execution engine to execute queries on data stored in Hadoop or other distributed storage systems. This mode is suitable for high-performance data processing and analysis on large-scale datasets.


7. Features and Limitations of Hive.
Ans:
Features of Hive:

SQL-like Interface: Hive provides a SQL-like interface that is familiar to most data analysts and developers, making it easy to use and learn.

Scalability: Hive is designed to handle large-scale data warehousing and analytical workloads on distributed storage systems like Hadoop Distributed File System (HDFS) and Amazon S3.

Extensible: Hive is highly extensible and can be easily integrated with other Hadoop components and applications such as Pig, HBase, and Spark.

Support for Multiple File Formats: Hive supports a wide range of file formats such as CSV, JSON, Avro, and Parquet, making it easy to work with data in various formats.

Data Partitioning: Hive supports data partitioning, which enables users to efficiently query large datasets by dividing them into smaller, more manageable parts.

Built-in Functions: Hive provides a wide range of built-in functions for data processing and analysis, including mathematical, string, date-time, and aggregate functions.

Limitations of Hive:

Limited Real-Time Processing: Hive is not designed for real-time processing and is not suitable for applications that require low-latency data processing.

Limited Support for Transactions: Hive does not provide full support for ACID transactions, which can be a limitation for applications that require transactional consistency.

Limited Indexing: Hive provides limited support for indexing, which can affect query performance on large datasets.

Complexity: Hive can be complex to set up and configure, and its query language can be difficult to master for users without SQL experience.

Performance Overhead: Hive's MapReduce execution engine can add overhead to query processing, making it slower than other query engines in some cases.

Limited Support for Streaming Data: Hive is not designed for processing streaming data and is not suitable for applications that require real-time analysis of data streams.

Overall, Hive is a powerful and versatile data warehousing and analytical tool that offers a wide range of features for working with large datasets. However, it also has some limitations, particularly in terms of real-time processing, transactions, and indexing.


8. How to create a Database in HIVE?
Ans:
To create a database in Hive, you can use the following syntax:

CREATE DATABASE database_name;

For example, to create a database named mydb, you can use the following command:

CREATE DATABASE mydb;


9. How to create a table in HIVE?
Ans:

To create a table in Hive, you can use the following syntax:

CREATE TABLE table_name (
   column1 data_type,
   column2 data_type,
   ...
   columnN data_type
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS textfile;


Here, replace table_name with the name of the table you want to create, and specify the columns and their data types in the parentheses.

For example, to create a table named employees with columns id, name, and salary, you can use the following command:

CREATE TABLE employees (
   id INT,
   name STRING,
   salary FLOAT
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS textfile;


After executing this command, Hive will create a new table named employees with the specified columns and data types. The ROW FORMAT DELIMITED clause specifies that the data is stored in delimited format, and the FIELDS TERMINATED BY ',' clause specifies that the fields are separated by commas. The STORED AS textfile clause specifies that the table is stored as a text file.



10.What do you mean by describe and describe extended and describe formatted with respect to database and table.
Ans:
In Hive, DESCRIBE is a command used to display information about databases, tables, and columns. There are three versions of the DESCRIBE command: DESCRIBE, DESCRIBE EXTENDED, and DESCRIBE FORMATTED, each with a different level of detail.

DESCRIBE: This command displays a list of columns in a table and their data types.
Syntax: DESCRIBE table_name;

DESCRIBE EXTENDED: This command displays additional information about the table, such as the table location, input format, output format, and serialization format.
Syntax: DESCRIBE EXTENDED table_name;


DESCRIBE FORMATTED: This command displays information about the table in a more readable and formatted way, including the table properties, columns, partition information, and storage information.
Syntax: DESCRIBE FORMATTED table_name;

11.How to skip header rows from a table in Hive?
Ans:

In Hive, you can skip header rows from a table while querying by using the TBLPROPERTIES clause.

12.What is a hive operator? What are the different types of hive operators?
Ans:

In Hive, an operator is a symbol or a keyword that performs an operation on one or more expressions or values. Hive supports several types of operators:

Arithmetic operators: These operators perform basic arithmetic operations such as addition, subtraction, multiplication, and division. Examples include +, -, *, and /.

Comparison operators: These operators compare two values and return a Boolean value (true or false) based on the comparison. Examples include =, !=, >, <, >=, and <=.

Logical operators: These operators perform logical operations on Boolean values (true or false). Examples include AND, OR, and NOT.

Bitwise operators: These operators perform bitwise operations on integer values. Examples include &, |, ^, and ~.

String operators: These operators perform operations on string values such as concatenation (||) and pattern matching (LIKE).

Conditional operators: These operators provide a way to perform conditional logic in Hive. The CASE operator is a common example of a conditional operator.

Assignment operators: These operators assign a value to a variable or a column in a table. The = operator is the most common assignment operator in Hive.

Hive operators are used in HiveQL queries to perform various operations on data. They can be combined to create complex expressions and statements.


13.Explain about the Hive Built-In Functions
Ans:

Hive provides a wide range of built-in functions that can be used in HiveQL queries to perform various operations on data. Here are some of the most commonly used categories of built-in functions in Hive:

Mathematical functions: These functions perform mathematical operations such as ABS, CEIL, FLOOR, EXP, LOG, POWER, ROUND, and SQRT.

String functions: These functions perform operations on string values such as CONCAT, LENGTH, LOWER, UPPER, REPLACE, SUBSTR, TRIM, and REGEXP.

Date and time functions: These functions perform operations on date and time values such as CURRENT_DATE, CURRENT_TIMESTAMP, DATE_ADD, DATE_SUB, DATEDIFF, FROM_UNIXTIME, TO_DATE, and YEAR.

Conditional functions: These functions provide a way to perform conditional logic in Hive. Examples include CASE, IF, COALESCE, NULLIF, and NVL.

Aggregation functions: These functions perform calculations on a group of rows and return a single value. Examples include COUNT, SUM, AVG, MIN, MAX, and GROUP_CONCAT.

Collection functions: These functions perform operations on arrays and maps. Examples include ARRAY, MAP, SIZE, SORT_ARRAY, MAP_KEYS, and MAP_VALUES.

Type conversion functions: These functions convert data from one data type to another. Examples include CAST, TO_BOOLEAN, TO_DOUBLE, TO_INT, TO_STRING, and FROM_JSON.

Hive's built-in functions make it easier to perform complex operations on data in Hive. They can be combined with operators and other functions to create powerful queries that extract valuable insights from large datasets.


14. Write hive DDL and DML commands.

Ans:


DDL (Data Definition Language) and DML (Data Manipulation Language) are two types of commands used in Hive. Here are some examples of common DDL and DML commands in Hive:

DDL Commands:

CREATE DATABASE: Used to create a new database in Hive. Syntax: CREATE DATABASE database_name;

USE DATABASE: Used to switch to a particular database in Hive. Syntax: USE database_name;

CREATE TABLE: Used to create a new table in Hive. 

ALTER TABLE: Used to modify the structure of an existing table in Hive. 


DROP TABLE: Used to delete a table in Hive. Syntax: DROP TABLE table_name;

DML Commands:

INSERT INTO: Used to insert new records into a table in Hive. 

SELECT: Used to retrieve data from one or more tables in Hive.


UPDATE: Used to update existing records in a table in Hive


DELETE: Used to delete records from a table in Hive.


15.Explain about SORT BY, ORDER BY, DISTRIBUTE BY and CLUSTER BY in Hive.

Ans:

In Hive, SORT BY, ORDER BY, DISTRIBUTE BY, and CLUSTER BY are used to organize data within a query result set. Here's a brief explanation of each:

SORT BY: Used to sort data within each reducer. SORT BY does not affect the number of reducers used in the query. Syntax: SELECT column1, column2, ... FROM table_name SORT BY column1 ASC|DESC, column2 ASC|DESC, ...;

ORDER BY: Used to sort data across all reducers. ORDER BY may cause performance issues for large datasets as it requires a shuffle operation to sort the data. Syntax: SELECT column1, column2, ... FROM table_name ORDER BY column1 ASC|DESC, column2 ASC|DESC, ...;

DISTRIBUTE BY: Used to distribute data among reducers based on the values of one or more columns. It does not guarantee a specific order of the data within each reducer. Syntax: SELECT column1, column2, ... FROM table_name DISTRIBUTE BY column1, column2, ...;

CLUSTER BY: Used to distribute and sort data within each reducer based on the values of one or more columns. It guarantees the order of the data within each reducer. CLUSTER BY is equivalent to using both DISTRIBUTE BY and SORT BY together. Syntax: SELECT column1, column2, ... FROM table_name CLUSTER BY column1, column2, ...;

In summary, SORT BY, ORDER BY, DISTRIBUTE BY, and CLUSTER BY are used to sort and organize data within a query result set in different ways. The choice of which one to use depends on the specific requirements of the query and the characteristics of the data being queried.

16.Difference between "Internal Table" and "External Table" and Mention when to choose “Internal Table” and “External Table” in Hive?

Ans:

In Hive, there are two types of tables - internal tables and external tables. Here are the differences between the two:

Internal Table: In an internal table, the data and metadata are stored in the Hive Warehouse directory managed by Hive. Internal tables are tightly coupled with Hive, and if you drop an internal table, it deletes the data and metadata associated with it.

External Table: In an external table, the metadata is stored in Hive, but the data is stored outside of the Hive Warehouse directory, typically in HDFS or another distributed file system. External tables are loosely coupled with Hive, and dropping an external table does not delete the data associated with it.

When to choose "Internal Table" and "External Table" in Hive:

Internal tables should be used when the data is managed by Hive, and you don't need to share the data with other systems. Internal tables are best suited for structured data.

External tables should be used when you need to share the data with other systems or you don't want Hive to manage the data. External tables are best suited for unstructured or semi-structured data, such as log files, which can be accessed by multiple applications.

In summary, internal tables are tightly coupled with Hive, and external tables are loosely coupled with Hive. The choice between the two depends on the specific requirements of the use case and the characteristics of the data being stored.


17.Where does the data of a Hive table get stored?
Ans :

In Hive, the data of a table can be stored in either of the following ways:

In the Hadoop Distributed File System (HDFS): This is the most common way to store data in Hive. In this case, the data is stored in one or more HDFS files, and Hive manages the metadata for the table.

In another distributed file system: Hive supports other distributed file systems besides HDFS, such as Amazon S3 and Microsoft Azure Data Lake Storage. In this case, the data is stored in the external file system, and Hive manages the metadata for the table.

The location of the data for a Hive table is specified in the table's DDL (Data Definition Language) statement using the LOCATION keyword. If the location is not specified, Hive uses its default location in the HDFS.

In summary, the data for a Hive table can be stored in HDFS or another distributed file system, and the location is specified in the table's DDL statement. Hive manages the metadata for the table, regardless of where the data is stored.




18.Is it possible to change the default location of a managed table?
Ans:
Yes, it is possible to change the default location of a managed table in Hive.

19.What is a metastore in Hive? What is the default database provided by Apache Hive for metastore?

Ans:
In Hive, the metastore is a central repository that stores metadata about the Hive tables, such as their schema, location, and format. The metastore acts as a bridge between Hive and the underlying storage system, such as HDFS or another distributed file system, by managing the mapping between the Hive tables and their corresponding data files.

By default, Apache Hive provides an embedded Derby database as the metastore. 


20.Why does Hive not store metadata information in HDFS?
Ans:
Hive does not store metadata information in HDFS because HDFS is designed to handle large data files and is optimized for sequential read/write operations. On the other hand, metadata is small in size and requires frequent random access operations for efficient querying and processing of the Hive tables.


21.What is a partition in Hive? And Why do we perform partitioning in Hive?
Ans:
In Hive, a partition is a way of dividing a table into smaller, more manageable parts based on a specific column value. Each partition is stored as a separate directory or file in HDFS, which makes it easier to manage and query large datasets.

Partitioning in Hive is performed for the following reasons:

Improved query performance: Partitioning allows Hive to process and retrieve only the relevant data for a query, rather than scanning the entire table. This can significantly improve query performance, especially for large datasets.

Better data organization: Partitioning allows you to organize data in a more logical and meaningful way based on specific business requirements, such as by date, region, or category. This makes it easier to manage and query data, and can also improve data processing and analysis.

Faster data loading: Partitioning can help to speed up the process of loading data into Hive. By partitioning data based on specific criteria, you can load data in parallel, which can significantly reduce the time required to load large datasets.


22.What is the difference between dynamic partitioning and static partitioning?
Ans:
The main difference between dynamic partitioning and static partitioning in Hive is how partitions are created and managed.

Static Partitioning: In static partitioning, the partition values are specified by the user while creating the table. The partition columns are defined as part of the table schema, and partitions are created before data is loaded into the table. Static partitioning requires prior knowledge of the partitioning scheme and values, which makes it less flexible compared to dynamic partitioning.

Dynamic Partitioning: In dynamic partitioning, partitions are created automatically based on the values of the partition columns in the data. When data is loaded into a table with dynamic partitioning, Hive scans the data and identifies distinct partition column values, and then creates the corresponding partitions in HDFS. Dynamic partitioning is more flexible than static partitioning, as it does not require prior knowledge of the partition values.


23.How do you check if a particular partition exists?
Ans:

To check if a particular partition exists in a Hive table, you can use the SHOW PARTITIONS command

24.How can you stop a partition form being queried?
Ans:

In Hive, you can set a partition to be offline, which prevents it from being queried. This can be useful when you need to perform maintenance on a partition or when you want to exclude a partition from queries for some other reason.

ALTER TABLE my_table PARTITION (my_partition='value') SET OFFLINE;


25.Why do we need buckets? How Hive distributes the rows into buckets?
Ans:
Buckets are a way of organizing data in Hive tables. They provide an additional level of physical organization beyond partitions. Buckets group data into files based on a hash function applied to the bucketing column. This allows queries to be more efficient, as only the relevant buckets need to be read


There are several reasons why you might want to use buckets in Hive:

Efficient sampling: If you need to sample a large dataset, buckets can help to reduce the amount of data that needs to be read.

Efficient joins: If you are joining two tables on a common column, bucketing the tables on that column can improve the performance of the join operation.

Efficient sorting: If you need to sort a large dataset, bucketing can help to reduce the amount of data that needs to be sorted.


26.In Hive, how can you enable buckets?

Ans:

To enable buckets in Hive, you can use the CLUSTERED BY and SORTED BY clauses when creating a table.

The CLUSTERED BY clause is used to specify the column(s) on which the data should be bucketed. The SORTED BY clause is used to specify the column(s) by which the data should be sorted within each bucket.

Here's an example of how to create a table with buckets enabled:

CREATE TABLE my_table (
  id INT,
  name STRING,
  age INT
)
CLUSTERED BY (id) INTO 4 BUCKETS
SORTED BY (name) ;

27.How does bucketing help in the faster execution of queries?
Ans:

Bucketing in Hive helps in the faster execution of queries in several ways:

Reduced Data Scanning: When a query is executed on a table with bucketing, Hive scans only the required buckets instead of scanning the entire table. This reduces the amount of data that needs to be read and processed, leading to faster query execution.

Data Locality: Bucketing helps to improve data locality. By grouping data with similar values into the same bucket, Hive can ensure that the data is stored physically close to each other on the disk. This makes it possible to process the data more efficiently, as the processing nodes can read the data directly from the disk, rather than having to transfer it across the network.

28.How to optimise Hive Performance? Explain in very detail.
Ans:
Hive is a powerful tool for processing large amounts of data, but as the size of the data grows, it can become slow and inefficient. Here are some tips for optimizing Hive performance:

Use appropriate hardware: Hive is designed to run on commodity hardware, but performance can be improved by using high-performance servers with large amounts of memory and fast disks.

Optimize Hive Configuration: The configuration parameters in Hive can be adjusted to improve performance. For example, increasing the number of mappers and reducers can improve parallelism, while increasing the buffer size can improve data transfer rates.

Partitioning: Hive partitioning can improve query performance by limiting the amount of data that needs to be scanned. By partitioning data based on a specific column, queries can be executed on a smaller subset of data.

Bucketing: Bucketing can further improve query performance by grouping data with similar values into the same bucket. This can help Hive to execute queries more efficiently by reducing the amount of data that needs to be scanned.

Compression: Compressing data can significantly reduce the amount of data that needs to be transferred and processed. Hive supports a number of compression codecs, such as Gzip and Snappy.

Use appropriate file formats: Choosing the appropriate file format can also have a significant impact on performance. For example, the ORC (Optimized Row Columnar) file format is designed for high performance and can improve query execution time.

Use Indexing: Hive supports indexing on specific columns which can improve the performance of queries that filter data on those columns.

Use Vectorization: Hive Vectorization improves performance by processing data in batches instead of row by row.

Use Caching: Hive supports caching, which can significantly reduce the time required to execute frequently executed queries.

Avoid using SELECT *: Query performance can be improved by avoiding the use of SELECT * and instead only selecting the required columns.

Avoid using Complex Queries: Complex queries with multiple joins and subqueries can significantly degrade performance. Try to simplify queries as much as possible.

In summary, optimizing Hive performance requires a combination of hardware and software configuration, partitioning, compression, appropriate file formats, indexing, vectorization, caching, and careful query design. By following these best practices, you can improve the performance of Hive and process large amounts of data more efficiently.





29. What is the use of Hcatalog?
Ans:
Some of the key use cases of HCatalog are:

Schema Management: HCatalog provides a unified schema for all data stored in HDFS, which can be used by different data processing tools.

Data Sharing: HCatalog enables data sharing between different tools such as Pig, Hive, and MapReduce, without the need to move data between tools.

Security: HCatalog provides fine-grained access control to data stored in Hadoop, allowing administrators to restrict access to specific data sets.

Simplified Data Processing: HCatalog simplifies data processing by providing a single layer of abstraction for managing data in Hadoop, making it easier for data analysts and developers to work with large amounts of data.

30. Explain about the different types of join in Hive.
Ans:
Inner Join: This is the most common type of join in which only the matching records from both tables are returned. In other words, it returns only the rows where there is a match in both tables.

Left Outer Join: In a left outer join, all the records from the left table are returned, along with matching records from the right table. If there is no matching record in the right table, null values are returned.

Right Outer Join: A right outer join returns all the records from the right table, along with matching records from the left table. If there is no matching record in the left table, null values are returned.

Full Outer Join: A full outer join returns all the records from both tables, along with null values for any records that do not have a match in the other table.

Left Semi Join: A left semi join returns only the records from the left table that have a match in the right table. It is similar to an inner join, but only the columns from the left table are returned.

Left Anti Join: A left anti join returns only the records from the left table that do not have a match in the right table. It is the opposite of a left semi join.

Cross Join: A cross join returns the Cartesian product of the two tables, i.e., it returns all possible combinations of rows from both tables. It can be useful for generating test data, but is not typically used in production.


31.Is it possible to create a Cartesian join between 2 tables, using Hive?

Ans:
Yes, it is possible to create a Cartesian join between 2 tables in Hive by using the CROSS JOIN keyword in the SELECT statement.



32.Explain the SMB Join in Hive?

Ans:
SMB (Sort Merge Bucketed) join is a specific type of join algorithm in Hive that is optimized for joining large tables by taking advantage of bucketing and sorting.

SMB join consists of two stages: the first stage involves sorting the tables on the join key and the second stage involves merging the sorted tables using the join key. The sorting operation is performed on the data stored in buckets, which makes it more efficient.





33.What is the difference between order by and sort by which one we should use?
Ans:

In Hive, both ORDER BY and SORT BY are used to sort the data within a table, but they work differently.

ORDER BY sorts the entire dataset by the specified column(s) and returns the result, while SORT BY sorts the dataset only within each reducer task and returns the result.

If you need a globally sorted result, then you should use ORDER BY. However, if you need to sort the data only within each reducer, or you have a small dataset that can fit into memory, then SORT BY can be more efficient.


34.What is the usefulness of the DISTRIBUTED BY clause in Hive?
Ans:
The DISTRIBUTE BY clause in Hive is used to specify the column or columns on which the data should be partitioned or grouped by, before the data is sent to the reducers.
When a DISTRIBUTE BY clause is used, Hive ensures that all the records that have the same value for the specified column(s) go to the same reducer. This can help to reduce the amount of data movement and improve performance in certain scenarios, especially when there is skew in the data distribution.


35.How does data transfer happen from HDFS to Hive?
Ans: 
Hive uses the Hadoop Distributed File System (HDFS) to store its data. When data is loaded into Hive tables, it is first stored in HDFS. Hive then uses the Hadoop MapReduce framework to process the data and generate query results.


36.Wherever (Different Directory) I run the hive query, it creates a new metastore_db, please explain the reason for it?
Ans:
The metastore_db is a directory that stores metadata information about tables, databases, columns, partitions, and other objects created in Hive. It is created by the embedded Apache Derby database which is used as the default metastore for Hive. Whenever a new Hive query is run from a different directory, a new metastore_db directory is created as Hive looks for the default Derby database in the current directory. This is because the metastore_db directory is not shared across different directories.

37.What will happen in case you have not issued the command: ‘SET hive.enforce.bucketing=true;’ before bucketing a table in Hive?
Ans :
If the command 'SET hive.enforce.bucketing=true;' is not issued before bucketing a table in Hive, the table may still be bucketed but the bucketing property will not be enforced. This can result in data skewness and slower query performance as the query planner may not be able to optimize the query based on the assumption of uniform distribution of data across buckets. 


38.Can a table be renamed in Hive?
Ans:
Yes, a table can be renamed in Hive using the RENAME TABLE statement. The syntax for renaming a table is:

RENAME TABLE old_table_name TO new_table_name;

39.Write a query to insert a new column(new_col INT) into a hive table at a position before an existing column (x_col)
Ans:
To insert a new column(new_col INT) into a Hive table at a position before an existing column (x_col), you need to use the ALTER TABLE statement with the CHANGE command. The syntax for the query is:
ALTER TABLE table_name CHANGE x_col x_col INT FIRST, new_col INT AFTER x_col;

This query will add the new column at the position just before the x_col column.



40.What is serde operation in HIVE?
Ans:

SerDe stands for Serializer/Deserializer. It is a framework in Hive that is used to serialize and deserialize data between Hadoop and Hive tables. SerDe allows Hive to read data from non-text files such as Avro, RCFile, ORC, and Parquet. It also allows Hive to store data in different formats than the default text format. SerDe provides a way to specify the schema of the data in a file and how to interpret the bytes in the file as fields in the schema. Hive supports a variety of built-in SerDe libraries such as LazySimpleSerDe, AvroSerDe, OrcSerDe, and ParquetSerDe, and also allows for custom SerDe libraries to be written.


41.Explain how Hive Deserializes and serialises the data?
Ans:
Hive deserializes data by reading data from the input source and converting it into a structured format that can be used by Hive. Serialization is the process of converting this structured format back into a format that can be written to an output source.


42.Write the name of the built-in serde in hive.
Ans:

The built-in serde in Hive is called "LazySimpleSerDe".

43.What is the need of custom Serde?
Ans:

Custom SerDe is needed when the built-in SerDe is not suitable for the input data format. It allows Hive to read and write data in a custom format and can be used to support new data formats that are not supported by the built-in SerDe.


44.Can you write the name of a complex data type(collection data types) in Hive?
Ans:
ARRAY
MAP
STRUCT


45.Can hive queries be executed from script files? How?
Ans:
Yes, hive queries can be executed from script files using the command hive -f script_name


46.What are the default record and field delimiter used for hive text files?
Ans:
The default record delimiter used for hive text files is '\n' (newline), and the default field delimiter is '\t' (tab).


47.How do you list all databases in Hive whose name starts with s?
Ans:
SHOW DATABASES LIKE 's*';


48.What is the difference between LIKE and RLIKE operators in Hive?
Ans:
LIKE and RLIKE are operators used for pattern matching in Hive. The main difference between them is that LIKE uses simple regular expressions while RLIKE uses extended regular expressions.


49.How to change the column data type in Hive?
Ans:
ALTER TABLE table_name CHANGE column_name column_name_new_data_type;



50.How will you convert the string ’51.2’ to a float value in the particular column?
Ans:
SELECT CAST(column_name AS FLOAT) FROM table_name;



51.What will be the result when you cast ‘abc’ (string) as INT?
Ans:

When you cast 'abc' (string) as INT, it will result in a NULL value since 'abc' cannot be converted to an integer.


52.What does the following query do?
a. INSERT OVERWRITE TABLE employees
b. PARTITION (country, state)
c. SELECT ..., se.cnty, se.st
d. FROM staged_employees se;

Ans:
The given query will insert data into the "employees" table in Hive while overwriting any existing data. The data will be partitioned based on two columns - "country" and "state". The select statement following the "PARTITION" keyword will select the columns to be inserted into the "employees" table along with the "country" and "state" columns from the "staged_employees" table. The "staged_employees" table is likely a temporary or staging table containing the data to be inserted into the "employees" table.


53.Write a query where you can overwrite data in a new table from the existing table.
Ans:
To overwrite data in a new table from the existing table in Hive, you can use the INSERT OVERWRITE TABLE statement followed by the SELECT statement that retrieves data from the existing table. For example

INSERT OVERWRITE TABLE new_table
SELECT * FROM existing_table;



54.What is the maximum size of a string data type supported by Hive? Explain how Hive supports binary formats.
Ans:

The maximum size of a string data type supported by Hive is 2^31-1 bytes (2 GB). Hive supports binary formats by using SerDe (Serializer/Deserializer) which allows Hive to read and write data in various formats such as Avro, Parquet, ORC, and others.


55. What File Formats and Applications Does Hive Support?
Ans:

Hive supports various file formats such as TextFile, SequenceFile, ORC, Avro, Parquet, and RCFile. It can also work with external applications such as Hadoop Distributed File System (HDFS), HBase, and Amazon S3.

56.How do ORC format tables help Hive to enhance its performance?
Ans:
ORC format tables help Hive to enhance its performance in several ways. ORC format stores data in a columnar manner, which reduces the I/O operations needed to read data. It also compresses data and uses predicate pushdown to filter data before it is read, which further improves performance.

57.How can Hive avoid mapreduce while processing the query?
Ans:
Hive can avoid mapreduce while processing a query by using Tez, a framework for fast and efficient data processing. Tez is an alternative to MapReduce that allows Hive to process queries faster by optimizing the execution plan and reducing the number of tasks.

58.What is view and indexing in hive?
Ans:
A view in Hive is a virtual table that does not physically store data. It is created by a SELECT statement that retrieves data from one or more tables. Indexing in Hive is a way to improve the performance of queries by creating an index on one or more columns of a table.


59.Can the name of a view be the same as the name of a hive table?
Ans:
Yes, the name of a view can be the same as the name of a Hive table.

60.What types of costs are associated in creating indexes on hive tables?
Ans:
The costs associated with creating indexes on Hive tables include the storage cost of the index, the cost of maintaining the index when data is inserted, updated or deleted, and the cost of querying the index to retrieve data.


61.Give the command to see the indexes on a table.
Ans:
To see the indexes on a table in Hive, you can use the DESCRIBE INDEX statement.

DESCRIBE INDEX my_index ON my_table;



62. Explain the process to access subdirectories recursively in Hive queries.
Ans:
To access subdirectories recursively in Hive queries, you can use the 'recursive' option with the Hadoop file system command. 
For example:
SELECT * FROM my_table
WHERE input_file_name() LIKE '/path/to/directory/*'
AND isdir('/path/to/directory', recursive=true);



63.If you run a select * query in Hive, why doesn't it run MapReduce?
Ans:
When you run a SELECT * query in Hive, it does not run MapReduce because Hive can retrieve the data directly from the file system without the need for MapReduce.



64.What are the uses of Hive Explode?
Ans:
Hive Explode is a function that is used to split a column that contains an array or a map into multiple rows. It is used to flatten a nested data structure and to enable further analysis of the data.


65. What is the available mechanism for connecting applications when we run Hive as a server?
Ans:
Hive can be run as a server using Thrift, a cross-language framework for building RPC clients and servers. Applications can connect to Hive as a server using Thrift and send queries to it.

66.Can the default location of a managed table be changed in Hive?
Ans:
Yes, the default location of a managed table can be changed in Hive by specifying a new location using the LOCATION clause in the CREATE TABLE statement. 
For example:
CREATE TABLE my_table (id INT, name STRING)
LOCATION '/new/path/to/table';



67.What is the Hive ObjectInspector function?
Ans:
The Hive ObjectInspector function is used to inspect the internal structure of objects in Hive. It is used by UDFs to access the data stored in a column and to perform operations on it.



68.What is UDF in Hive?
Ans:

A User-Defined Function (UDF) in Hive is a custom function that is created by a user to perform a specific task


69.Write a query to extract data from hdfs to hive.
Ans:
To extract data from HDFS to Hive, we can use the following query:
INSERT OVERWRITE TABLE <hive_table_name>
SELECT *
FROM <hdfs_directory_path>;




70.What is TextInputFormat and SequenceFileInputFormat in hive.
Ans:
TextInputFormat and SequenceFileInputFormat are file input formats supported by Hive. TextInputFormat is used for processing text files, while SequenceFileInputFormat is used for processing binary files in the Hadoop SequenceFile format.


71.How can you prevent a large job from running for a long time in a hive?
Ans:

We can prevent a large job from running for a long time in Hive by enabling dynamic partitioning and bucketing. Dynamic partitioning reduces the amount of data processed by each query, while bucketing partitions the data into smaller, more manageable chunks.


72.When do we use explode in Hive?
Ans:
We use explode in Hive to split an array or a map column into multiple rows, with each row containing a single element of the array or map. This is useful when we want to perform aggregations or filtering on individual elements of an array or map.


73.Can Hive process any type of data formats? Why? Explain in very detail
Ans:

Hive can process a wide variety of data formats, including text files, binary files in various formats, Avro files, Parquet files, ORC files, and more. This is because Hive leverages the input/output formats provided by Hadoop, which supports a large number of file formats.

74.Whenever we run a Hive query, a new metastore_db is created. Why?
Ans:
Whenever we run a Hive query, a new metastore_db is created to store the metadata associated with that query. This metadata includes information about the Hive tables being accessed, the location of the data files, and any transformations or processing being performed on the data.

75.Can we change the data type of a column in a hive table? Write a complete query.
Ans:
Yes, we can change the data type of a column in a Hive table using the ALTER TABLE command. For example, to change the data type of a column named col1 in a table named mytable from string to int, we can use the following query:

ALTER TABLE mytable CHANGE col1 col1 INT;



76.While loading data into a hive table using the LOAD DATA clause, how do you specify it is a hdfs file and not a local file ?
Ans:

To specify that the file being loaded into a Hive table using the LOAD DATA clause is an HDFS file and not a local file, we can use the LOCATION keyword followed by the HDFS path to the file. For example:
LOAD DATA INPATH '/user/hive/input/data.txt' OVERWRITE INTO TABLE mytable;




77.What is the precedence order in Hive configuration?
Ans:

The precedence order in Hive configuration is as follows:
Hive session-level settings (i.e., the SET command)
Hive user-level settings (stored in the user's .hiverc file)
Hive server-level settings (stored in hive-site.xml)
Hadoop configuration settings (stored in core-site.xml, hdfs-site.xml, etc.)

78.Which interface is used for accessing the Hive metastore?
Ans:

The interface used for accessing the Hive metastore is called the Hive Metastore API. This API provides a set of functions that allow clients to create, read, update, and delete metadata associated with Hive tables and other objects.


79.Is it possible to compress json in the Hive external table ?
Ans:
Yes, it is possible to compress JSON data in a Hive external table using the serde (serializer/deserializer) property. Specifically, we can use the org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe serde, which supports both columnar and compressed data formats.

80.What is the difference between local and remote metastores?
Ans:
The main difference between local and remote metastores in Hive is where the metadata is stored. A local metastore stores the metadata on the same machine as the Hive server, while a remote metastore stores the metadata on a separate machine that is dedicated to managing the metadata.


81.What is the purpose of archiving tables in Hive?
Ans:
The purpose of archiving tables in Hive is to move or copy data from one location to another while retaining its original state. Archiving tables can help in data backup, data recovery, and data migration between different Hadoop clusters. Hive provides built-in support for archiving tables using the ALTER TABLE command with the SET TBLPROPERTIES clause.



82.What is DBPROPERTY in Hive?
Ans:

In Hive, DBPROPERTY is a built-in function that is used to retrieve metadata information about a database. It is used to get the value of a property associated with the database. DBPROPERTY takes two arguments: the database name and the property name. For example, the following query can be used to get the description of a database:

SELECT DBPROPERTY('my_database', 'comment');


83.Differentiate between local mode and MapReduce mode in Hive
Ans:
Local mode and MapReduce mode are two execution modes supported by Hive.
Local mode is a non-distributed execution mode in which Hive executes queries on the local machine without using Hadoop MapReduce. In this mode, Hive runs as a single JVM process and data is read from and written to the local file system. Local mode is typically used for testing and debugging purposes and is not suitable for large-scale data processing.

MapReduce mode is a distributed execution mode in which Hive uses Hadoop MapReduce to process large volumes of data stored in HDFS. In this mode, Hive generates MapReduce jobs to execute queries on the Hadoop cluster. MapReduce mode is suitable for large-scale data processing and is the default execution mode in Hive. However, it may have higher latency compared to local mode due to the overhead of launching MapReduce jobs.







